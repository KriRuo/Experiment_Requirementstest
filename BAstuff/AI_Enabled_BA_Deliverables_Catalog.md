# AI-Enabled BA Deliverables Catalog

## Expert IT Business Analyst Deliverable Reference with AI Acceleration

This catalog provides a comprehensive guide to Business Analyst deliverables enriched with concrete AI acceleration techniques, prompts, and automation workflows.

---

## Complete Deliverable Catalog

| Deliverable | Problem(s) Solved | Description | Core Components | Quality Criteria | Typical Input Sources | Typical Consumers | Format Examples | AI Acceleration | Typical Mistakes | Review Checklist | When NOT to Use |
|-------------|-------------------|-------------|-----------------|------------------|----------------------|-------------------|-----------------|-----------------|------------------|------------------|-----------------|
| **Vision Canvas** | No aligned direction; Stakeholders have conflicting views; Team lacks focus; Strategy unclear | One-page strategic overview capturing target group, needs, solution, business goals, and success metrics | • Target group/personas<br>• Core problems/needs<br>• Proposed solution<br>• Business goals<br>• Success metrics<br>• Key differentiators | • Understandable in <2 minutes<br>• Aligned with business strategy<br>• Validated by key stakeholders<br>• Measurable success criteria<br>• Clear scope boundaries | • Executive interviews<br>• Strategy documents<br>• Market research<br>• Customer feedback<br>• Competitive analysis | • Executive sponsor<br>• Product owner<br>• Development team<br>• Sales/Marketing<br>• Investors | • Lean Canvas<br>• Business Model Canvas<br>• Product Vision Board<br>• One-pager template | **1. Generate First Draft:**<br>"Given these interview transcripts: [paste], synthesize a Vision Canvas with: target group, their top 3 problems, proposed solution, 3 business goals, and 5 measurable success criteria. Use practitioner language."<br><br>**2. Validate Quality:**<br>"Review this Vision Canvas against BABOK best practices. Check for: alignment between problems and solution, measurable goals, realistic scope, missing stakeholder perspectives. Provide gap analysis."<br><br>**3. Automation:**<br>"Extract key themes from these 5 stakeholder meeting notes [paste] and generate a draft Vision Canvas. Flag conflicting viewpoints and suggest resolution."<br><br>**4. Consistency Check:**<br>"Compare this Vision Canvas to the Product Backlog items. Identify backlog items that don't align with stated goals and flag orphaned objectives with no supporting backlog items."<br><br>**5. Critique Mode:**<br>"Act as a skeptical CFO. Challenge this Vision Canvas: Are goals measurable? Is ROI clear? Are risks acknowledged? What's missing?" | • Too generic/vague<br>• No measurable metrics<br>• Mixing vision with implementation<br>• Too complex (>1 page)<br>• Not validated with stakeholders | ☐ Fits on one page<br>☐ Target group clearly defined<br>☐ Goals are SMART<br>☐ Validated by sponsor<br>☐ Everyone understands it | • When strategy already clear and documented<br>• For small tactical changes<br>• When organization has mature strategy artifacts |
| **Stakeholder Map** | Unknown stakeholders; Communication breakdowns; Politics unclear; Unclear ownership; Risk of missing key players | Visual identification of all parties involved, their relationships, influence, interest, and engagement strategy | • Stakeholder names/roles<br>• Power/Interest classification<br>• Relationships/dependencies<br>• Communication preferences<br>• Engagement strategy per group<br>• RACI assignments | • No missing key stakeholders<br>• Current and accurate<br>• Engagement strategy per group<br>• Validated by project sponsor<br>• Clear ownership (RACI) | • Org charts<br>• Project charter<br>• Interview outputs<br>• Historical project data<br>• Process documentation | • Project manager<br>• BA team<br>• Change managers<br>• Executive sponsor<br>• Communication lead | • Power/Interest Grid<br>• Onion Diagram<br>• Stakeholder Register<br>• RACI Matrix | **1. Generate First Draft:**<br>"Based on this project description [paste] and org chart [paste], generate a comprehensive stakeholder list. For each: classify power (High/Medium/Low), interest (High/Medium/Low), role, and initial engagement strategy."<br><br>**2. Validate Quality:**<br>"Review this stakeholder map. Check for: missing stakeholder categories (users, operators, maintainers, regulators, budget holders), orphaned stakeholders with no engagement plan, conflicting RACI assignments."<br><br>**3. Automation:**<br>"Analyze these email threads and meeting invites [paste] to identify all project stakeholders not yet in our stakeholder map. Extract their roles and interaction patterns."<br><br>**4. Generate RACI:**<br>"Given this stakeholder list and these key activities [list], generate a RACI matrix. Flag where: multiple people are Accountable, no one is Accountable, everyone is Consulted (meeting overload)."<br><br>**5. Relationship Mapping:**<br>"Create a Mermaid diagram showing stakeholder relationships and dependencies from this stakeholder list. Highlight potential conflict points and alliance opportunities." | • Forgetting external stakeholders<br>• Static map never updated<br>• No engagement strategy<br>• Confusing interest with power<br>• Multiple owners (RACI) | ☐ All categories covered (users, business, tech, external)<br>☐ Engagement strategy defined<br>☐ RACI has single Accountable<br>☐ Updated within last month<br>☐ Validated by sponsor | • When stakeholder landscape is stable and well-known<br>• For very small internal projects<br>• When organizational politics are irrelevant |
| **Context Diagram** | System boundaries unclear; Integration points unknown; External dependencies hidden; Scope creep risk; Data flow confusion | High-level visualization showing system in its environment with actors, external systems, and interfaces | • System boundary<br>• External actors (users/systems)<br>• Data flows/interfaces<br>• Protocols/technologies<br>• Integration points<br>• Scope inclusions/exclusions | • No orphan interfaces<br>• All actors identified<br>• Clear boundary<br>• Bidirectional flows shown<br>• Validated by architects<br>• Consistent with requirements | • Architecture docs<br>• Integration specs<br>• Stakeholder interviews<br>• Technical documentation<br>• Legacy system docs | • Solution architects<br>• Development team<br>• Integration team<br>• Security team<br>• Operations | • C4 Context Diagram<br>• UML Context Diagram<br>• System Context Box<br>• Data Flow Context | **1. Generate First Draft:**<br>"From this requirements document [paste], generate a C4 Context Diagram in Mermaid format. Include: the system boundary, all external users and systems, data flows with labels (request/response type), and key protocols (REST, SOAP, etc.)."<br><br>**2. Validate Quality:**<br>"Review this Context Diagram for completeness. Check for: orphan actors with no interactions, missing integration points, unidirectional flows that should be bidirectional, actors that should be systems, unlabeled interfaces."<br><br>**3. Automation:**<br>"Analyze this API documentation and integration guide [paste]. Extract all external systems and generate a Context Diagram showing integration points, protocols, and data exchanged."<br><br>**4. Consistency Check:**<br>"Compare this Context Diagram with NFR specifications. Verify: all external integrations have performance NFRs, security requirements for each interface, monitoring points identified."<br><br>**5. Gap Analysis:**<br>"Review architecture discussions [paste transcripts]. Identify external actors or systems mentioned but missing from the Context Diagram. Suggest additions with rationale." | • Showing internal components (wrong level)<br>• Missing user types<br>• No technology labels<br>• Confusing context with container<br>• Not showing data flow direction | ☐ System boundary clear<br>☐ All external actors present<br>☐ Flows have directions and labels<br>☐ Technology/protocol noted<br>☐ Architect approved | • When system is entirely standalone<br>• For detailed internal architecture (use Container/Component diagrams)<br>• When boundaries frequently change |
| **BPMN Process Model** | Business process unclear; Handoffs undefined; Bottlenecks hidden; No process documentation; Automation opportunities unknown | Standardized visual notation for documenting business processes with activities, gateways, events, and flows | • Activities (tasks)<br>• Sequence flows<br>• Gateways (AND/OR/XOR)<br>• Events (start/end/intermediate)<br>• Lanes (roles/systems)<br>• Data objects | • BPMN 2.0 compliant<br>• Process is executable<br>• Exception paths included<br>• Happy and unhappy paths<br>• Validated by process owners<br>• Consistent level of detail | • Process interviews<br>• Process mining data<br>• Existing process docs<br>• Workflow systems<br>• Observation sessions | • Process owners<br>• Business stakeholders<br>• Developers<br>• Process automation team<br>• Auditors | • BPMN 2.0 XML<br>• Visual diagram (Camunda, Signavio)<br>• Swimlane diagram | **1. Generate First Draft:**<br>"Convert this process description into BPMN 2.0 format: [paste description]. Include: swim lanes for each role, decision gateways where choices are made, error handling paths, start and end events. Output as Mermaid diagram or BPMN XML."<br><br>**2. Validate Quality:**<br>"Analyze this BPMN model for issues: unreachable activities, missing error handlers, dead-end paths, excessive complexity (>20 activities - suggest subprocess decomposition), missing swimlane assignments."<br><br>**3. Automation from Transcript:**<br>"From this process workshop transcript [paste], extract: activities in sequence, decision points with conditions, roles responsible for each activity, exception scenarios. Generate BPMN model."<br><br>**4. Process Mining Integration:**<br>"Compare this as-designed BPMN model with process mining event log data [paste]. Identify: deviations from model, bottlenecks (long wait times), frequently skipped activities, suggest optimization opportunities."<br><br>**5. Automation Opportunity Finder:**<br>"Analyze this BPMN model and identify automation candidates. Flag: repetitive manual tasks, rule-based decisions, data entry activities, copy/paste operations. Estimate automation complexity and ROI." | • Too detailed (wrong abstraction)<br>• Missing exception paths<br>• No swimlanes (unclear ownership)<br>• Mixing AS-IS and TO-BE<br>• Non-BPMN notation | ☐ BPMN 2.0 compliant notation<br>☐ All paths reach an end<br>☐ Exception handling included<br>☐ Lanes assigned to roles<br>☐ Validated by process owner | • For very simple linear processes<br>• When process is ad-hoc and varies greatly<br>• For strategic (not operational) processes |
| **User Story Map** | No shared understanding of scope; Priorities unclear; Release planning impossible; User journey not visible; Big backlog chaos | Visual decomposition of user activities into a prioritized, release-sliced story map showing user backbone and story details | • User backbone (activities)<br>• User tasks per activity<br>• Story cards with details<br>• Priority ranking<br>• Release slices (MVP, v2, v3)<br>• Dependencies | • Complete user journey<br>• Stories are independent<br>• Release slices deliver value<br>• Team consensus on priorities<br>• All stories estimable | • User research<br>• Journey maps<br>• Product vision<br>• Stakeholder workshops<br>• Competitive analysis | • Product Owner<br>• Development team<br>• UX designers<br>• Business stakeholders<br>• QA team | • Physical story map (cards on wall)<br>• Digital tools (Miro, StoriesOnBoard)<br>• Spreadsheet with grouping | **1. Generate First Draft:**<br>"From this product vision [paste] and user personas [paste], generate a User Story Map. Create: 5-7 user activities (backbone), 3-5 tasks per activity, story cards in 'As a [user], I want [action] so that [benefit]' format. Organize left-to-right by user flow."<br><br>**2. Validate Quality:**<br>"Review this User Story Map for: gaps in user journey, stories that don't fit INVEST criteria, missing critical path stories, inconsistent level of detail, stories without clear user value."<br><br>**3. Release Slicing:**<br>"Given this story map and MVP definition [paste], suggest 3 horizontal release slices. Ensure: each slice delivers end-to-end value, MVP is smallest viable, dependencies are respected. Flag risky assumptions."<br><br>**4. Story Generation from Journey:**<br>"Convert this user journey map [paste] into user stories organized in story map format. Group by activity, ensure each story has acceptance criteria, estimate relative size (S/M/L)."<br><br>**5. Dependency Detection:**<br>"Analyze this story map and identify: blocking dependencies (must-do-first), technical enablers needed, stories that should be split, stories that can be done in parallel. Generate dependency diagram." | • Creating it alone (not workshop)<br>• Too many layers<br>• Stories too large<br>• No release slices<br>• Treating it as static | ☐ Full user journey covered<br>☐ Stories follow INVEST<br>☐ Release slices defined<br>☐ Team participated in creation<br>☐ Priorities clear | • When product scope is tiny<br>• For backend-only systems with no user journey<br>• When backlog is already well-understood |
| **Product Backlog** | No prioritized work list; Unclear what to build next; Team pulls wrong items; Stakeholder requests ad-hoc; No single source of truth | Ordered list of all known product work (features, enhancements, fixes, technical debt) maintained by Product Owner | • Backlog items (stories/epics)<br>• Priority order<br>• Acceptance criteria<br>• Size estimates<br>• Business value<br>• Dependencies<br>• Status | • Single ordered list<br>• Top items refined (ready)<br>• Sized appropriately<br>• Constantly prioritized<br>• Transparent to stakeholders<br>• Links to DoD/DoR | • Stakeholder requests<br>• User feedback<br>• Bug reports<br>• Technical debt assessment<br>• Strategic initiatives | • Development team<br>• Product Owner<br>• Scrum Master<br>• Stakeholders<br>• UX team | • JIRA/Azure DevOps<br>• Spreadsheet<br>• Physical board<br>• Backlog management tool | **1. Generate First Draft:**<br>"From this requirements document [paste] and stakeholder priorities [paste], generate a prioritized product backlog. Include: epic/story title, user story format, acceptance criteria (Given/When/Then), priority score, T-shirt size estimate. Order by business value."<br><br>**2. Validate Quality:**<br>"Review this product backlog for health issues: vague stories (missing AC), items not following user story format, sizing inconsistencies, items without clear value, duplicates, missing technical debt items."<br><br>**3. Priority Scoring:**<br>"Apply WSJF scoring to these backlog items [paste]. For each calculate: Business Value (1-10), Time Criticality (1-10), Risk/Opportunity (1-10), Job Size (1-10), WSJF score. Reorder by score and explain changes."<br><br>**4. Acceptance Criteria Generation:**<br>"For each user story in this backlog [paste], generate comprehensive acceptance criteria in Given/When/Then format. Include: happy path, edge cases, error scenarios, NFR criteria (performance, security)."<br><br>**5. Refinement Assistant:**<br>"Analyze top 10 backlog items for Definition of Ready compliance. Check: clear description, AC defined, dependencies identified, sized, no impediments, UX ready. Flag items needing refinement with specific gaps." | • Not ordered (grouped by type)<br>• Stale items never removed<br>• Bottom never refined<br>• Too detailed too early<br>• Mixing backlog and roadmap | ☐ Single ordered list<br>☐ Top 10 items meet DoR<br>☐ Recent items <10% of total<br>☐ Regularly groomed (last 2 weeks)<br>☐ All items have value statement | • When working from fixed specification<br>• For very short projects<br>• When priorities never change |
| **Use Case Specification** | System behavior unclear; Actor interactions undefined; Alternative flows missing; Functional requirements incomplete; Testability low | Detailed narrative describing system interactions with actors to achieve specific goals, including preconditions, main flow, alternatives, and postconditions | • Use case name and ID<br>• Actors (primary/secondary)<br>• Preconditions<br>• Main success scenario<br>• Alternative flows<br>• Postconditions<br>• Business rules | • All flows testable<br>• Preconditions clear<br>• Alternatives complete<br>• Actor goals met<br>• Traceable to requirements<br>• Peer reviewed | • Requirements documents<br>• User interviews<br>• Process models<br>• System specifications<br>• Domain models | • Developers<br>• Test engineers<br>• Business analysts<br>• Technical writers<br>• Architects | • Cockburn format template<br>• Rational Unified Process<br>• Fully-dressed use case<br>• SRS section | **1. Generate First Draft:**<br>"Create a fully-dressed use case from this requirement: [paste]. Include: use case name, primary/secondary actors, preconditions, main success scenario (numbered steps), alternative flows (labeled 3a, 3b), exception flows, postconditions, business rules."<br><br>**2. Validate Quality:**<br>"Review this use case specification for completeness: missing alternative flows, untestable preconditions, ambiguous steps, missing postconditions, no error handling, steps from system perspective instead of actor perspective."<br><br>**3. Test Case Generation:**<br>"From this use case [paste], generate comprehensive test cases covering: main flow test, each alternative flow, each exception scenario, boundary conditions, negative tests. Format as: Test ID, Description, Steps, Expected Result."<br><br>**4. Scenario Expansion:**<br>"Analyze this main success scenario [paste] and generate all plausible alternative flows. For each step ask: what could go wrong? What variations exist? What if concurrent users? Label as 1a, 1b, 2a, etc."<br><br>**5. Consistency Check:**<br>"Compare these 5 use cases [paste] for consistency issues: same business rule stated differently, duplicate scenarios, conflicting preconditions, missing shared alternative flows, terminology inconsistencies." | • Missing alternative flows<br>• Too technical (not business view)<br>• Mixing design with requirements<br>• No error handling<br>• Too abstract or too detailed | ☐ All preconditions testable<br>☐ Main flow and alternatives complete<br>☐ Business rules documented<br>☐ Written from actor perspective<br>☐ Peer reviewed | • For simple CRUD operations<br>• When user stories are sufficient<br>• In highly agile environments preferring stories |
| **Data Model** | Data structure unknown; Entity relationships unclear; Data integrity risks; Database design undefined; Integration contracts vague | Visual and textual representation of data entities, attributes, relationships, and constraints (conceptual, logical, or physical) | • Entities<br>• Attributes and data types<br>• Primary/Foreign keys<br>• Relationships and cardinality<br>• Constraints and rules<br>• Data dictionary | • Normalized (or justified denormalization)<br>• All relationships defined<br>• Constraints documented<br>• Consistent naming<br>• Validated by data owners<br>• Technical review passed | • Requirements docs<br>• Domain models<br>• Legacy database schemas<br>• Integration specs<br>• Business rules | • Database developers<br>• Backend developers<br>• Data architects<br>• Integration team<br>• BI team | • ER Diagram<br>• UML Class Diagram<br>• Physical schema (DDL)<br>• Data dictionary spreadsheet | **1. Generate First Draft:**<br>"From these requirements [paste], create a logical data model. Generate: entities with attributes and data types, primary keys, relationships with cardinality (1:1, 1:N, N:M), foreign keys. Output as ER diagram in Mermaid format and data dictionary table."<br><br>**2. Validate Quality:**<br>"Review this data model for issues: normalization violations (redundancy), missing primary keys, orphan entities, many-to-many not resolved with junction table, inconsistent naming conventions, missing constraints."<br><br>**3. Data Dictionary Generation:**<br>"From this ER diagram [paste] or database schema [paste], generate a complete data dictionary. For each attribute: entity, attribute name, data type, length, nullable, default value, description, business rules, sample values."<br><br>**4. Reverse Engineering:**<br>"Analyze this database DDL script [paste] and generate: conceptual ER diagram, entity descriptions, relationship explanations, identified design patterns, potential refactoring suggestions."<br><br>**5. Integration Check:**<br>"Compare this data model with API contracts [paste]. Verify: all API request/response objects map to entities, data types compatible, required fields align, identify mapping gaps and recommend resolution." | • Not normalizing (redundancy)<br>• Confusing conceptual/logical/physical<br>• Missing data dictionary<br>• No cardinality specified<br>• Premature optimization | ☐ Properly normalized (3NF or justified)<br>☐ All PKs and FKs defined<br>☐ Cardinality specified<br>☐ Data dictionary complete<br>☐ Technical review passed | • When using NoSQL without fixed schema<br>• For prototype/proof-of-concept<br>• When data structure is extremely simple |
| **NFR Catalog** | Quality attributes ignored; Performance issues in production; Security vulnerabilities; Scalability problems; No testable quality criteria | Comprehensive specification of non-functional requirements organized by categories (FURPS+: Functionality, Usability, Reliability, Performance, Supportability, plus constraints) | • NFR categories (FURPS+)<br>• Specific measurable criteria<br>• Priority (must/should/could)<br>• Rationale<br>• Verification method<br>• Acceptance thresholds | • Measurable and testable<br>• Prioritized<br>• Technically feasible<br>• Traceable to scenarios<br>• Reviewed by architects<br>• Includes acceptance criteria | • Quality workshops<br>• SLAs<br>• Regulatory requirements<br>• Architecture decisions<br>• Production incidents | • Solution architects<br>• Development team<br>• Security team<br>• Operations/SRE<br>• QA team | • NFR specification doc<br>• Quality Attribute Scenarios<br>• Architecture Decision Records<br>• SLA documents | **1. Generate First Draft:**<br>"Create a comprehensive NFR catalog for this system [paste description]. Cover FURPS+ categories: Performance (response times, throughput), Scalability (users, data volume), Availability (uptime %), Security (authentication, authorization, encryption), Usability (accessibility, learnability), Maintainability (code quality, documentation). Make each measurable."<br><br>**2. Validate Quality:**<br>"Review this NFR catalog for issues: vague requirements ('fast', 'secure'), untestable criteria, missing categories (no security or availability), no priorities, missing verification methods, unrealistic targets, no rationale provided."<br><br>**3. Quality Attribute Scenarios:**<br>"Convert these NFRs [paste] into Quality Attribute Scenarios using format: Source → Stimulus → Artifact → Environment → Response → Response Measure. Example: 'User submits form → System → Normal load → Response < 2 sec, 95th percentile'."<br><br>**4. Consistency Check:**<br>"Compare NFRs with architecture decisions [paste ADRs] and test plan [paste]. Identify: NFRs without architecture support, NFRs without tests, conflicting NFRs (security vs performance), missing traceability."<br><br>**5. Gap Analysis from Incidents:**<br>"Analyze these production incident reports [paste]. Identify: missing NFRs that would have prevented issues, existing NFRs not met, new NFR categories needed, suggest specific measurable requirements." | • Vague terms ('fast', 'secure')<br>• Not measurable<br>• Forgetting constraints<br>• No priorities<br>• Discovered too late | ☐ All categories covered (FURPS+)<br>☐ Each NFR is measurable<br>☐ Priorities assigned<br>☐ Verification method defined<br>☐ Architect approved | • For throwaway prototypes<br>• When inheriting well-defined platform NFRs<br>• For trivial internal tools |
| **Acceptance Criteria** | Done means different things to people; Testing unclear; Scope ambiguity; User expectations undefined; Regression risks | Concrete, testable conditions that must be satisfied for a work item to be considered complete and acceptable | • Scenario descriptions<br>• Given-When-Then format<br>• Positive and negative cases<br>• Edge cases<br>• NFR criteria<br>• Test data examples | • Testable and verifiable<br>• Unambiguous<br>• Independent of implementation<br>• Covers happy and unhappy paths<br>• Reviewed with team and PO | • User stories<br>• Use cases<br>• Stakeholder conversations<br>• Business rules<br>• Regulatory requirements | • Development team<br>• QA team<br>• Product Owner<br>• Automated test frameworks<br>• Stakeholders | • Gherkin (BDD)<br>• Bullet list in story<br>• Checklist<br>• Example tables | **1. Generate First Draft:**<br>"From this user story [paste], generate comprehensive acceptance criteria in Given-When-Then (Gherkin) format. Include: main scenario, alternative scenarios (at least 2), error/exception handling, boundary conditions, NFR criteria (performance, security, usability where relevant)."<br><br>**2. Validate Quality:**<br>"Review these acceptance criteria for gaps: missing negative tests, no edge cases, vague conditions ('appropriate', 'reasonable'), implementation details instead of behavior, missing NFR aspects, untestable criteria."<br><br>**3. Test Case Generation:**<br>"Convert these acceptance criteria [paste] into executable test cases. For each scenario: Test ID, test steps (arrange-act-assert), test data, expected result, automation priority (high/med/low)."<br><br>**4. Scenario Expansion:**<br>"Analyze this acceptance criteria [paste] using boundary value analysis and equivalence partitioning. Generate additional test scenarios for: boundary conditions, invalid inputs, concurrent access, error conditions, timeout scenarios."<br><br>**5. Consistency Check:**<br>"Compare acceptance criteria across these related stories [paste]. Identify: inconsistent terminology, duplicate scenarios, missing scenarios appearing in related stories, conflicting validation rules." | • Too implementation-specific<br>• Not testable<br>• Missing negative cases<br>• Too many in one story<br>• Created after development | ☐ Given-When-Then format<br>☐ Happy path and exceptions covered<br>☐ All testable<br>☐ Agreed by team and PO<br>☐ Independent of implementation | • For spike stories (research)<br>• For technical tasks with clear completion<br>• When story is self-evident |
| **Prototype / Wireframe** | UI/UX unclear; User flow uncertain; Requirements validation needed; Usability assumptions untested; Design alignment lacking | Visual representation of interface structure, layout, interactions, and user flow at varying fidelities (lo-fi sketches to hi-fi interactive prototypes) | • Screen layouts<br>• Navigation flow<br>• UI components and states<br>• Interactions and transitions<br>• Content hierarchy<br>• Annotations | • Appropriate fidelity for stage<br>• Covers key user flows<br>• Annotated with behavior<br>• Tested with users<br>• Addresses feedback<br>• Aligned with brand/style | • User stories<br>• Journey maps<br>• User research<br>• Business requirements<br>• Competitive analysis | • UX designers<br>• Developers<br>• Product Owner<br>• Stakeholders<br>• End users (testing) | • Sketches (paper/digital)<br>• Wireframes (Balsamiq)<br>• Mockups (Figma, Sketch)<br>• Interactive prototypes (InVision) | **1. Generate First Draft:**<br>"From these user stories [paste] and user flow description [paste], generate wireframe descriptions for each screen. For each screen describe: layout structure (header/content/footer), key UI components (forms, buttons, lists), navigation elements, content sections, interactions. Suggest screen state variations."<br><br>**2. Validate Quality:**<br>"Review these wireframes/prototype against usability heuristics: visibility of system status, match to real world, user control, consistency, error prevention, recognition over recall, flexibility, aesthetic and minimalist, error recovery, help and documentation. Flag violations with severity."<br><br>**3. User Flow Generation:**<br>"From this prototype screens [paste screen descriptions], generate a user flow diagram in Mermaid. Show: entry points, decision points, screen transitions, success/error paths, exit points. Identify incomplete flows or dead ends."<br><br>**4. Annotation Extraction:**<br>"Review this prototype [paste screenshots or descriptions] and generate comprehensive annotation list: interaction behaviors (on-click, on-hover), validation rules, error messages, loading states, empty states, permission-based variations, responsive behavior."<br><br>**5. Consistency Check:**<br>"Compare wireframes with user stories [paste] and acceptance criteria [paste]. Verify: all stories have screen coverage, all AC elements visible in prototype, identify stories not addressed, flag design elements not in requirements." | • Too high fidelity too early<br>• Not testing with users<br>• Designing in isolation<br>• Skipping lo-fi validation<br>• Treating prototype as specification | ☐ Fidelity matches project stage<br>☐ Key flows covered<br>☐ Tested with users<br>☐ Feedback incorporated<br>☐ Annotations clear | • When UI is standard/template-based<br>• For backend-only systems<br>• When design is already validated |
| **Impact Analysis** | Change consequences unknown; Risk of unintended effects; Dependencies unclear; Effort underestimated; Stakeholder communication gaps | Systematic assessment of change effects across technical components, processes, people, and data to plan mitigation and communication | • Affected components/systems<br>• Impacted stakeholders<br>• Changed processes<br>• Data/integration impacts<br>• Risk assessment<br>• Mitigation plans | • All affected areas identified<br>• Impact severity rated<br>• Dependencies mapped<br>• Risks with mitigation<br>• Stakeholders informed<br>• Effort estimated | • Change requests<br>• Architecture docs<br>• Requirements traceability<br>• System documentation<br>• Stakeholder map | • Change manager<br>• Project manager<br>• Development team<br>• Stakeholders<br>• Test team | • Impact analysis report<br>• Dependency diagram<br>• Traceability matrix<br>• Risk register update | **1. Generate First Draft:**<br>"Analyze impact of this change: [paste change description]. Given these artifacts [paste architecture, requirements, etc.], identify impacts on: technical components, integrations, data model, business processes, users, existing features. Rate each impact (High/Med/Low) with rationale."<br><br>**2. Validate Quality:**<br>"Review this impact analysis for completeness. Check for missing impact areas: security implications, performance impact, backward compatibility, migration needs, training requirements, documentation updates, test coverage, operations impact."<br><br>**3. Dependency Analysis:**<br>"Given this proposed change [paste] and requirements traceability matrix [paste], identify: dependent requirements, blocking dependencies, reverse dependencies (what depends on changed items), cascade effects. Generate dependency graph."<br><br>**4. Stakeholder Impact:**<br>"From this impact analysis [paste] and stakeholder map [paste], determine: which stakeholders affected, how they're impacted, communication needs, training requirements, timeline for notification. Generate stakeholder communication plan."<br><br>**5. Test Impact:**<br>"Analyze this change [paste] against test suite [paste test plan/cases]. Identify: tests that need updating, new test scenarios required, regression test areas, test data changes, test environment impacts, estimated test effort." | • Only analyzing technical impact<br>• Forgetting data migration<br>• Not considering operations<br>• Underestimating ripple effects<br>• No mitigation plans | ☐ All impact areas covered (technical, process, people, data)<br>☐ Severity ratings assigned<br>☐ Dependencies identified<br>☐ Mitigation defined<br>☐ Stakeholders identified | • For trivial isolated changes<br>• When change is entirely new (no modifications)<br>• In proof-of-concept stage |
| **Business Case** | Investment justification unclear; ROI unknown; Alternatives not evaluated; Cost-benefit not quantified; Executive buy-in lacking | Economic and strategic justification for initiative, including problem statement, solution options, cost-benefit analysis, risk assessment, and recommendation | • Problem/opportunity statement<br>• Options analysis<br>• Cost breakdown (CapEx/OpEx)<br>• Benefit quantification<br>• ROI/NPV/Payback<br>• Risk assessment<br>• Recommendation | • Financially sound<br>• Multiple options evaluated<br>• Assumptions documented<br>• Risks identified<br>• Aligned with strategy<br>• Approved by finance | • Business strategy<br>• Market analysis<br>• Cost estimates<br>• Benefit projections<br>• Risk assessments<br>• Benchmarking data | • Executive sponsor<br>• Finance team<br>• Steering committee<br>• Investment board<br>• Program manager | • Business case document<br>• Financial model (Excel)<br>• Executive summary<br>• Options matrix | **1. Generate First Draft:**<br>"Create a business case structure from this initiative description [paste]. Include: executive summary, problem statement with quantified pain points, 3 solution options (including do-nothing), cost breakdown (development, licensing, operations, training), benefit categories (cost savings, revenue increase, risk reduction), 5-year financial projection, recommendation with rationale."<br><br>**2. Validate Quality:**<br>"Review this business case for weaknesses: unquantified benefits, missing cost categories (training, maintenance, opportunity cost), optimistic assumptions, no sensitivity analysis, risks without financial impact, no success metrics, missing strategic alignment statement."<br><br>**3. Financial Calculation:**<br>"Given these costs [paste] and benefits [paste], calculate: Year-by-year cash flow (Years 0-5), NPV (discount rate 10%), IRR, Payback period, Break-even point, ROI percentage. Create summary table and explain which metric matters most for this initiative."<br><br>**4. Sensitivity Analysis:**<br>"Perform sensitivity analysis on this business case [paste financials]. Vary key assumptions by ±20%: development cost, user adoption rate, benefit realization timeline, discount rate. Identify most sensitive variables and risk to ROI. Recommend assumption validation activities."<br><br>**5. Comparison to Benchmarks:**<br>"Compare this business case [paste] to industry benchmarks for [type of initiative]. Assess if: costs are reasonable, benefits are realistic, timeline is achievable, ROI meets typical ranges. Flag outliers and suggest adjustments or justifications." | • Only showing benefits, not costs<br>• No do-nothing option<br>• Ignoring intangible benefits<br>• Unrealistic timelines<br>• No risk to benefits | ☐ Options include do-nothing<br>☐ All costs included (TCO)<br>☐ Benefits quantified where possible<br>☐ Financial metrics calculated (NPV, ROI)<br>☐ Risks assessed | • For mandatory compliance projects (still track costs)<br>• For very small investments below approval threshold<br>• When business case already exists |
| **Definition of Ready** | Stories not ready for sprint; Development blocked; Requirements incomplete; Team pulls unprepared work; Velocity suffers | Checklist of criteria that user stories/backlog items must meet before being pulled into a sprint or development cycle | • INVEST criteria (Independent, Negotiable, Valuable, Estimable, Small, Testable)<br>• Acceptance criteria defined<br>• Dependencies resolved<br>• Designs available<br>• Estimate agreed<br>• Team understanding confirmed | • Agreed by whole team<br>• Consistently applied<br>• Reviewed regularly<br>• Visible and accessible<br>• Enforced at sprint planning<br>• Reduces mid-sprint surprises | • Team retrospectives<br>• Sprint planning feedback<br>• Historical blockers<br>• DoD as reference<br>• Team working agreements | • Product Owner<br>• Scrum Master<br>• Development team<br>• UX team<br>• QA team | • Checklist (wiki, board)<br>• Story template with mandatory fields<br>• Confluence page<br>• Team room poster | **1. Generate First Draft:**<br>"Create a Definition of Ready checklist for a [describe team/domain]. Include criteria for: story clarity (INVEST), acceptance criteria completeness, design readiness, technical feasibility reviewed, dependencies identified and resolved, estimate provided, test approach defined, no blockers present."<br><br>**2. Validate Quality:**<br>"Review this Definition of Ready [paste] for issues: too many criteria (>10 items), vague criteria not verifiable, missing key areas (designs, dependencies, estimates), criteria checking implementation details, not adapted to team context, overlaps with DoD."<br><br>**3. Automated Compliance Check:**<br>"Check these backlog items [paste] against Definition of Ready criteria [paste DoR]. For each item report: compliant (yes/no), which criteria failed, specific gaps to address, readiness percentage, recommended actions to make ready."<br><br>**4. Historical Analysis:**<br>"Analyze sprint blockers and interruptions from last 3 sprints [paste data]. Identify: root causes, which DoR criteria would have caught these, suggest new DoR criteria, propose updates to existing DoR."<br><br>**5. Comparative Analysis:**<br>"Compare our DoR [paste] with INVEST principles and industry best practices. Identify: missing standard criteria, overly strict criteria impacting flow, good criteria to keep, suggested additions/removals with rationale." | • Too many criteria (bureaucracy)<br>• Not enforced<br>• Criteria too vague<br>• Created without team<br>• Never updated | ☐ Agreed by whole team<br>☐ Criteria are checkable<br>☐ Applied consistently<br>☐ Under 10 items<br>☐ Reviewed last quarter | • When team very experienced and self-organizing<br>• For prototyping/spike work<br>• When specifications are always complete |
| **Test Concept** | Test strategy unclear; Coverage gaps; Test types undefined; Environment needs unknown; Quality risks unmitigated | Comprehensive test strategy document defining scope, approach, types, levels, environment, data, schedule, and exit criteria | • Test scope and objectives<br>• Test levels (unit/integration/system/acceptance)<br>• Test types (functional/NFR/regression)<br>• Test environment requirements<br>• Test data strategy<br>• Entry/Exit criteria<br>• Roles and responsibilities | • Comprehensive coverage<br>• Risk-based prioritization<br>• Environment defined<br>• Roles clear<br>• Aligned with schedule<br>• Approved by QA lead | • Requirements specs<br>• Architecture docs<br>• NFR catalog<br>• Risk register<br>• Project schedule<br>• Quality standards | • QA team<br>• Test managers<br>• Developers<br>• DevOps team<br>• Project manager | • Test plan document (IEEE 829)<br>• Test strategy<br>• Test approach matrix<br>• Test pyramid diagram | **1. Generate First Draft:**<br>"Create a comprehensive test concept for [paste project description]. Include: in-scope/out-of-scope, test levels (unit, integration, system, UAT) with % coverage targets, test types (functional, performance, security, usability), environment topology, test data approach, automation strategy (what to automate), entry/exit criteria, risks to quality."<br><br>**2. Validate Quality:**<br>"Review this test concept for gaps: missing test levels, no NFR testing (performance, security), undefined environment, no test data strategy, no automation plan, unclear entry/exit criteria, roles not assigned, no risk-based prioritization, unrealistic schedule."<br><br>**3. Test Case Generation:**<br>"From requirements [paste] and this test concept [paste], generate master test case list. For each requirement: test scenarios needed, test level (unit/integration/system), test type, priority (P1/P2/P3), automation candidate (Y/N), estimated effort."<br><br>**4. Coverage Analysis:**<br>"Analyze test coverage: given requirements traceability matrix [paste] and test cases [paste], identify: requirements without tests, orphan tests, coverage gaps by priority (ensure all critical requirements tested), NFR without test approach."<br><br>**5. Environment and Data Planning:**<br>"Based on this test concept [paste] and system architecture [paste], detail: test environments needed (dev/test/staging/prod), configuration for each, test data requirements per test level, data refresh strategy, environment provisioning timeline, infrastructure costs." | • Only functional testing<br>• No NFR testing<br>• Environment as afterthought<br>• No automation strategy<br>• Ignoring test data needs | ☐ All test levels defined<br>☐ NFR testing included<br>☐ Environment specified<br>☐ Automation strategy clear<br>☐ Approved by QA lead | • For trivial changes<br>• When following existing test strategy<br>• For proof-of-concept work |
| **Release Plan** | Release timing unclear; Scope uncertainty; Dependency coordination lacking; Rollback plan missing; Go-live chaos | Coordinated schedule for delivering product increments, defining scope per release, dependencies, milestones, go-live activities, and rollback procedures | • Release schedule and milestones<br>• Scope per release<br>• Dependencies and sequencing<br>• Go-live activities checklist<br>• Rollback procedures<br>• Success metrics per release | • Realistic timeline<br>• Dependencies managed<br>• Risk mitigation planned<br>• Stakeholders aligned<br>• Rollback defined<br>• Success metrics clear | • Product roadmap<br>• Story map<br>• Backlog<br>• Architecture constraints<br>• Team capacity<br>• Dependency maps | • Product Owner<br>• Development team<br>• Release manager<br>• Operations<br>• Stakeholders | • Release calendar<br>• Roadmap (timeline)<br>• Release checklist<br>• Communication plan | **1. Generate First Draft:**<br>"Create a release plan from this backlog [paste] and team capacity [paste]. Define: 3 releases (MVP, v2, v3) with dates, scope per release (epics/stories), why each release delivers value, dependencies between releases, key milestones (code freeze, testing, go-live), high-level timeline (Gantt)."<br><br>**2. Validate Quality:**<br>"Review this release plan for issues: releases don't deliver standalone value, unrealistic timeline for scope, dependencies not sequenced correctly, no buffer for risks, missing go-live activities (deployment, training, cutover), no rollback plan, success metrics undefined."<br><br>**3. Dependency Sequencing:**<br>"Analyze these stories/epics [paste] and their dependencies [paste]. Generate optimal release sequencing that: minimizes dependencies between releases, delivers value incrementally, highlights critical path, identifies parallelization opportunities, suggests release boundary adjustments."<br><br>**4. Go-Live Checklist Generation:**<br>"Create a comprehensive go-live checklist for release [name]. Include: pre-deployment (backups, notification), deployment steps (sequence, rollback points), validation steps (smoke tests), post-deployment (monitoring, support readiness), communication (users, stakeholders), rollback procedure."<br><br>**5. Release Risk Analysis:**<br>"Analyze risks to this release plan [paste]. Identify: scope creep risks, dependency delays, technical risks (integration, performance), organizational risks (approvals, resources), suggest mitigation strategies, recommend release readiness criteria." | • Too much scope in MVP<br>• No incremental value<br>• Dependencies ignored<br>• No rollback plan<br>• Dates without capacity check | ☐ Each release delivers value<br>☐ Dependencies sequenced<br>☐ Timeline realistic for capacity<br>☐ Go-live checklist defined<br>☐ Rollback plan documented | • When releasing continuously with CI/CD<br>• For single-release projects<br>• When release process is fully standardized |

---

## How to Use This Catalog

1. **Select Deliverable** - Choose the artifact that matches your current BA task
2. **Review Context** - Understand problems solved, inputs, and consumers
3. **Check Quality Criteria** - Know what "good" looks like
4. **Apply AI Acceleration** - Use the concrete prompts and automation ideas to accelerate your work
5. **Validate Quality** - Use the review checklist before finalizing
6. **Avoid Pitfalls** - Check typical mistakes and when NOT to use

## AI Acceleration Patterns

The AI techniques in this catalog follow these proven patterns:

- **"Generate X from Y"** - Draft creation from source materials
- **"Review for gaps against checklist"** - Quality validation
- **"Convert meeting transcript → artifact"** - Meeting notes extraction
- **"Create Mermaid/BPMN from description"** - Diagram generation
- **"Create acceptance criteria from story"** - Derivation and expansion
- **Consistency checks between artifacts** - Cross-artifact validation
- **Test case generation** - From requirements to tests
- **Traceability creation** - Link requirements to implementation

## Key Principles

- **Practitioner Language**: Uses BABOK, IREB, and Agile terminology
- **Actionable Components**: Each element can be directly executed
- **Executable AI Ideas**: Prompts are copy-paste ready with clear inputs/outputs
- **Quality Focus**: Each deliverable includes quality criteria and review checklists
- **Context Awareness**: Guidance on when to use and when NOT to use each artifact

---

*This catalog is a living document. As AI capabilities evolve and new patterns emerge, update the acceleration techniques to reflect current best practices.*
