# AI-Enabled BA Deliverables Catalog

## Expert IT Business Analyst Deliverable Reference with AI Acceleration

This catalog provides a comprehensive guide to Business Analyst deliverables enriched with concrete AI acceleration techniques, prompts, and automation workflows.

---

## Complete Deliverable Catalog

|  Deliverable  |  Problem(s) Solved  |  Description  |  Core Components  |  Quality Criteria  |  Typical Input Sources  |  Typical Consumers  |  Format Examples  |  Typical Mistakes  |  Review Checklist  |  When NOT to Use  |
| ------------- | ------------------- | ------------- | ----------------- | ------------------ | ---------------------- | ------------------- | ----------------- | ------------------ | ------------------ | ----------------- |
|  **Vision Canvas**  |  No aligned direction; Stakeholders have conflicting views; Team lacks focus; Strategy unclear  |  One-page strategic overview capturing target group, needs, solution, business goals, and success metrics  |  • Target group/personas<br>• Core problems/needs<br>• Proposed solution<br>• Business goals<br>• Success metrics<br>• Key differentiators  |  • Understandable in <2 minutes<br>• Aligned with business strategy<br>• Validated by key stakeholders<br>• Measurable success criteria<br>• Clear scope boundaries  |  • Executive interviews<br>• Strategy documents<br>• Market research<br>• Customer feedback<br>• Competitive analysis  |  • Executive sponsor<br>• Product owner<br>• Development team<br>• Sales/Marketing<br>• Investors  |  • Lean Canvas<br>• Business Model Canvas<br>• Product Vision Board<br>• One-pager template  |  • Too generic/vague<br>• No measurable metrics<br>• Mixing vision with implementation<br>• Too complex (>1 page)<br>• Not validated with stakeholders  |  ☐ Fits on one page<br>☐ Target group clearly defined<br>☐ Goals are SMART<br>☐ Validated by sponsor<br>☐ Everyone understands it  |  • When strategy already clear and documented<br>• For small tactical changes<br>• When organization has mature strategy artifacts  |
|  **Stakeholder Map**  |  Unknown stakeholders; Communication breakdowns; Politics unclear; Unclear ownership; Risk of missing key players  |  Visual identification of all parties involved, their relationships, influence, interest, and engagement strategy  |  • Stakeholder names/roles<br>• Power/Interest classification<br>• Relationships/dependencies<br>• Communication preferences<br>• Engagement strategy per group<br>• RACI assignments  |  • No missing key stakeholders<br>• Current and accurate<br>• Engagement strategy per group<br>• Validated by project sponsor<br>• Clear ownership (RACI)  |  • Org charts<br>• Project charter<br>• Interview outputs<br>• Historical project data<br>• Process documentation  |  • Project manager<br>• BA team<br>• Change managers<br>• Executive sponsor<br>• Communication lead  |  • Power/Interest Grid<br>• Onion Diagram<br>• Stakeholder Register<br>• RACI Matrix  |  • Forgetting external stakeholders<br>• Static map never updated<br>• No engagement strategy<br>• Confusing interest with power<br>• Multiple owners (RACI)  |  ☐ All categories covered (users, business, tech, external)<br>☐ Engagement strategy defined<br>☐ RACI has single Accountable<br>☐ Updated within last month<br>☐ Validated by sponsor  |  • When stakeholder landscape is stable and well-known<br>• For very small internal projects<br>• When organizational politics are irrelevant  |
|  **Context Diagram**  |  System boundaries unclear; Integration points unknown; External dependencies hidden; Scope creep risk; Data flow confusion  |  High-level visualization showing system in its environment with actors, external systems, and interfaces  |  • System boundary<br>• External actors (users/systems)<br>• Data flows/interfaces<br>• Protocols/technologies<br>• Integration points<br>• Scope inclusions/exclusions  |  • No orphan interfaces<br>• All actors identified<br>• Clear boundary<br>• Bidirectional flows shown<br>• Validated by architects<br>• Consistent with requirements  |  • Architecture docs<br>• Integration specs<br>• Stakeholder interviews<br>• Technical documentation<br>• Legacy system docs  |  • Solution architects<br>• Development team<br>• Integration team<br>• Security team<br>• Operations  |  • C4 Context Diagram<br>• UML Context Diagram<br>• System Context Box<br>• Data Flow Context  |  • Showing internal components (wrong level)<br>• Missing user types<br>• No technology labels<br>• Confusing context with container<br>• Not showing data flow direction  |  ☐ System boundary clear<br>☐ All external actors present<br>☐ Flows have directions and labels<br>☐ Technology/protocol noted<br>☐ Architect approved  |  • When system is entirely standalone<br>• For detailed internal architecture (use Container/Component diagrams)<br>• When boundaries frequently change  |
|  **BPMN Process Model**  |  Business process unclear; Handoffs undefined; Bottlenecks hidden; No process documentation; Automation opportunities unknown  |  Standardized visual notation for documenting business processes with activities, gateways, events, and flows  |  • Activities (tasks)<br>• Sequence flows<br>• Gateways (AND/OR/XOR)<br>• Events (start/end/intermediate)<br>• Lanes (roles/systems)<br>• Data objects  |  • BPMN 2.0 compliant<br>• Process is executable<br>• Exception paths included<br>• Happy and unhappy paths<br>• Validated by process owners<br>• Consistent level of detail  |  • Process interviews<br>• Process mining data<br>• Existing process docs<br>• Workflow systems<br>• Observation sessions  |  • Process owners<br>• Business stakeholders<br>• Developers<br>• Process automation team<br>• Auditors  |  • BPMN 2.0 XML<br>• Visual diagram (Camunda, Signavio)<br>• Swimlane diagram  |  • Too detailed (wrong abstraction)<br>• Missing exception paths<br>• No swimlanes (unclear ownership)<br>• Mixing AS-IS and TO-BE<br>• Non-BPMN notation  |  ☐ BPMN 2.0 compliant notation<br>☐ All paths reach an end<br>☐ Exception handling included<br>☐ Lanes assigned to roles<br>☐ Validated by process owner  |  • For very simple linear processes<br>• When process is ad-hoc and varies greatly<br>• For strategic (not operational) processes  |
|  **User Story Map**  |  No shared understanding of scope; Priorities unclear; Release planning impossible; User journey not visible; Big backlog chaos  |  Visual decomposition of user activities into a prioritized, release-sliced story map showing user backbone and story details  |  • User backbone (activities)<br>• User tasks per activity<br>• Story cards with details<br>• Priority ranking<br>• Release slices (MVP, v2, v3)<br>• Dependencies  |  • Complete user journey<br>• Stories are independent<br>• Release slices deliver value<br>• Team consensus on priorities<br>• All stories estimable  |  • User research<br>• Journey maps<br>• Product vision<br>• Stakeholder workshops<br>• Competitive analysis  |  • Product Owner<br>• Development team<br>• UX designers<br>• Business stakeholders<br>• QA team  |  • Physical story map (cards on wall)<br>• Digital tools (Miro, StoriesOnBoard)<br>• Spreadsheet with grouping  |  • Creating it alone (not workshop)<br>• Too many layers<br>• Stories too large<br>• No release slices<br>• Treating it as static  |  ☐ Full user journey covered<br>☐ Stories follow INVEST<br>☐ Release slices defined<br>☐ Team participated in creation<br>☐ Priorities clear  |  • When product scope is tiny<br>• For backend-only systems with no user journey<br>• When backlog is already well-understood  |
|  **Product Backlog**  |  No prioritized work list; Unclear what to build next; Team pulls wrong items; Stakeholder requests ad-hoc; No single source of truth  |  Ordered list of all known product work (features, enhancements, fixes, technical debt) maintained by Product Owner  |  • Backlog items (stories/epics)<br>• Priority order<br>• Acceptance criteria<br>• Size estimates<br>• Business value<br>• Dependencies<br>• Status  |  • Single ordered list<br>• Top items refined (ready)<br>• Sized appropriately<br>• Constantly prioritized<br>• Transparent to stakeholders<br>• Links to DoD/DoR  |  • Stakeholder requests<br>• User feedback<br>• Bug reports<br>• Technical debt assessment<br>• Strategic initiatives  |  • Development team<br>• Product Owner<br>• Scrum Master<br>• Stakeholders<br>• UX team  |  • JIRA/Azure DevOps<br>• Spreadsheet<br>• Physical board<br>• Backlog management tool  |  • Not ordered (grouped by type)<br>• Stale items never removed<br>• Bottom never refined<br>• Too detailed too early<br>• Mixing backlog and roadmap  |  ☐ Single ordered list<br>☐ Top 10 items meet DoR<br>☐ Recent items <10% of total<br>☐ Regularly groomed (last 2 weeks)<br>☐ All items have value statement  |  • When working from fixed specification<br>• For very short projects<br>• When priorities never change  |
|  **Backlog Item**  |  Feature scope unclear; Business value not articulated; Success criteria undefined; Missing context for implementation; Stakeholder alignment lacking  |  Feature-sized work item with comprehensive description, business justification, acceptance criteria, and supporting context (diagrams, wireframes) for development planning  |  • Description (what and why)<br>• Business case/value proposition<br>• Acceptance criteria (Given-When-Then)<br>• Attached context (diagrams, wireframes, references)<br>• Size estimate<br>• Dependencies<br>• Priority/value score  |  • Meets INVEST criteria<br>• Business value clearly articulated<br>• Testable acceptance criteria<br>• Supporting diagrams attached<br>• Stakeholder validated<br>• Meets Definition of Ready  |  • Product vision<br>• User research<br>• Stakeholder workshops<br>• Story mapping sessions<br>• Technical feasibility analysis<br>• Market/competitor analysis  |  • Product Owner<br>• Development team<br>• UX designers<br>• QA team<br>• Architects<br>• Business stakeholders  |  • Epic/Feature in JIRA<br>• Azure DevOps Feature<br>• Story card with attachments<br>• Feature specification document  |  • Too large (not splittable)<br>• No business case<br>• Vague acceptance criteria<br>• Missing context diagrams<br>• Technical solution instead of need<br>• Not estimable by team  |  ☐ Description includes 'what' and 'why'<br>☐ Business case quantifies value<br>☐ AC covers happy and edge cases<br>☐ Context diagrams attached<br>☐ Team can estimate and commit<br>☐ Dependencies identified  |  • When requirement is trivial/obvious<br>• For technical spikes (use spike format)<br>• When detailed use case already exists<br>• For bug fixes (use defect format)  |
|  **Use Case Specification**  |  System behavior unclear; Actor interactions undefined; Alternative flows missing; Functional requirements incomplete; Testability low  |  Detailed narrative describing system interactions with actors to achieve specific goals, including preconditions, main flow, alternatives, and postconditions  |  • Use case name and ID<br>• Actors (primary/secondary)<br>• Preconditions<br>• Main success scenario<br>• Alternative flows<br>• Postconditions<br>• Business rules  |  • All flows testable<br>• Preconditions clear<br>• Alternatives complete<br>• Actor goals met<br>• Traceable to requirements<br>• Peer reviewed  |  • Requirements documents<br>• User interviews<br>• Process models<br>• System specifications<br>• Domain models  |  • Developers<br>• Test engineers<br>• Business analysts<br>• Technical writers<br>• Architects  |  • Cockburn format template<br>• Rational Unified Process<br>• Fully-dressed use case<br>• SRS section  |  • Missing alternative flows<br>• Too technical (not business view)<br>• Mixing design with requirements<br>• No error handling<br>• Too abstract or too detailed  |  ☐ All preconditions testable<br>☐ Main flow and alternatives complete<br>☐ Business rules documented<br>☐ Written from actor perspective<br>☐ Peer reviewed  |  • For simple CRUD operations<br>• When user stories are sufficient<br>• In highly agile environments preferring stories  |
|  **Data Model**  |  Data structure unknown; Entity relationships unclear; Data integrity risks; Database design undefined; Integration contracts vague  |  Visual and textual representation of data entities, attributes, relationships, and constraints (conceptual, logical, or physical)  |  • Entities<br>• Attributes and data types<br>• Primary/Foreign keys<br>• Relationships and cardinality<br>• Constraints and rules<br>• Data dictionary  |  • Normalized (or justified denormalization)<br>• All relationships defined<br>• Constraints documented<br>• Consistent naming<br>• Validated by data owners<br>• Technical review passed  |  • Requirements docs<br>• Domain models<br>• Legacy database schemas<br>• Integration specs<br>• Business rules  |  • Database developers<br>• Backend developers<br>• Data architects<br>• Integration team<br>• BI team  |  • ER Diagram<br>• UML Class Diagram<br>• Physical schema (DDL)<br>• Data dictionary spreadsheet  |  • Not normalizing (redundancy)<br>• Confusing conceptual/logical/physical<br>• Missing data dictionary<br>• No cardinality specified<br>• Premature optimization  |  ☐ Properly normalized (3NF or justified)<br>☐ All PKs and FKs defined<br>☐ Cardinality specified<br>☐ Data dictionary complete<br>☐ Technical review passed  |  • When using NoSQL without fixed schema<br>• For prototype/proof-of-concept<br>• When data structure is extremely simple  |
|  **NFR Catalog**  |  Quality attributes ignored; Performance issues in production; Security vulnerabilities; Scalability problems; No testable quality criteria  |  Comprehensive specification of non-functional requirements organized by categories (FURPS+: Functionality, Usability, Reliability, Performance, Supportability, plus constraints)  |  • NFR categories (FURPS+)<br>• Specific measurable criteria<br>• Priority (must/should/could)<br>• Rationale<br>• Verification method<br>• Acceptance thresholds  |  • Measurable and testable<br>• Prioritized<br>• Technically feasible<br>• Traceable to scenarios<br>• Reviewed by architects<br>• Includes acceptance criteria  |  • Quality workshops<br>• SLAs<br>• Regulatory requirements<br>• Architecture decisions<br>• Production incidents  |  • Solution architects<br>• Development team<br>• Security team<br>• Operations/SRE<br>• QA team  |  • NFR specification doc<br>• Quality Attribute Scenarios<br>• Architecture Decision Records<br>• SLA documents  |  • Vague terms ('fast', 'secure')<br>• Not measurable<br>• Forgetting constraints<br>• No priorities<br>• Discovered too late  |  ☐ All categories covered (FURPS+)<br>☐ Each NFR is measurable<br>☐ Priorities assigned<br>☐ Verification method defined<br>☐ Architect approved  |  • For throwaway prototypes<br>• When inheriting well-defined platform NFRs<br>• For trivial internal tools  |
|  **Acceptance Criteria**  |  Done means different things to people; Testing unclear; Scope ambiguity; User expectations undefined; Regression risks  |  Concrete, testable conditions that must be satisfied for a work item to be considered complete and acceptable  |  • Scenario descriptions<br>• Given-When-Then format<br>• Positive and negative cases<br>• Edge cases<br>• NFR criteria<br>• Test data examples  |  • Testable and verifiable<br>• Unambiguous<br>• Independent of implementation<br>• Covers happy and unhappy paths<br>• Reviewed with team and PO  |  • User stories<br>• Use cases<br>• Stakeholder conversations<br>• Business rules<br>• Regulatory requirements  |  • Development team<br>• QA team<br>• Product Owner<br>• Automated test frameworks<br>• Stakeholders  |  • Gherkin (BDD)<br>• Bullet list in story<br>• Checklist<br>• Example tables  |  • Too implementation-specific<br>• Not testable<br>• Missing negative cases<br>• Too many in one story<br>• Created after development  |  ☐ Given-When-Then format<br>☐ Happy path and exceptions covered<br>☐ All testable<br>☐ Agreed by team and PO<br>☐ Independent of implementation  |  • For spike stories (research)<br>• For technical tasks with clear completion<br>• When story is self-evident  |
|  **Prototype / Wireframe**  |  UI/UX unclear; User flow uncertain; Requirements validation needed; Usability assumptions untested; Design alignment lacking  |  Visual representation of interface structure, layout, interactions, and user flow at varying fidelities (lo-fi sketches to hi-fi interactive prototypes)  |  • Screen layouts<br>• Navigation flow<br>• UI components and states<br>• Interactions and transitions<br>• Content hierarchy<br>• Annotations  |  • Appropriate fidelity for stage<br>• Covers key user flows<br>• Annotated with behavior<br>• Tested with users<br>• Addresses feedback<br>• Aligned with brand/style  |  • User stories<br>• Journey maps<br>• User research<br>• Business requirements<br>• Competitive analysis  |  • UX designers<br>• Developers<br>• Product Owner<br>• Stakeholders<br>• End users (testing)  |  • Sketches (paper/digital)<br>• Wireframes (Balsamiq)<br>• Mockups (Figma, Sketch)<br>• Interactive prototypes (InVision)  |  • Too high fidelity too early<br>• Not testing with users<br>• Designing in isolation<br>• Skipping lo-fi validation<br>• Treating prototype as specification  |  ☐ Fidelity matches project stage<br>☐ Key flows covered<br>☐ Tested with users<br>☐ Feedback incorporated<br>☐ Annotations clear  |  • When UI is standard/template-based<br>• For backend-only systems<br>• When design is already validated  |
|  **Impact Analysis**  |  Change consequences unknown; Risk of unintended effects; Dependencies unclear; Effort underestimated; Stakeholder communication gaps  |  Systematic assessment of change effects across technical components, processes, people, and data to plan mitigation and communication  |  • Affected components/systems<br>• Impacted stakeholders<br>• Changed processes<br>• Data/integration impacts<br>• Risk assessment<br>• Mitigation plans  |  • All affected areas identified<br>• Impact severity rated<br>• Dependencies mapped<br>• Risks with mitigation<br>• Stakeholders informed<br>• Effort estimated  |  • Change requests<br>• Architecture docs<br>• Requirements traceability<br>• System documentation<br>• Stakeholder map  |  • Change manager<br>• Project manager<br>• Development team<br>• Stakeholders<br>• Test team  |  • Impact analysis report<br>• Dependency diagram<br>• Traceability matrix<br>• Risk register update  |  • Only analyzing technical impact<br>• Forgetting data migration<br>• Not considering operations<br>• Underestimating ripple effects<br>• No mitigation plans  |  ☐ All impact areas covered (technical, process, people, data)<br>☐ Severity ratings assigned<br>☐ Dependencies identified<br>☐ Mitigation defined<br>☐ Stakeholders identified  |  • For trivial isolated changes<br>• When change is entirely new (no modifications)<br>• In proof-of-concept stage  |
|  **Business Case**  |  Investment justification unclear; ROI unknown; Alternatives not evaluated; Cost-benefit not quantified; Executive buy-in lacking  |  Economic and strategic justification for initiative, including problem statement, solution options, cost-benefit analysis, risk assessment, and recommendation  |  • Problem/opportunity statement<br>• Options analysis<br>• Cost breakdown (CapEx/OpEx)<br>• Benefit quantification<br>• ROI/NPV/Payback<br>• Risk assessment<br>• Recommendation  |  • Financially sound<br>• Multiple options evaluated<br>• Assumptions documented<br>• Risks identified<br>• Aligned with strategy<br>• Approved by finance  |  • Business strategy<br>• Market analysis<br>• Cost estimates<br>• Benefit projections<br>• Risk assessments<br>• Benchmarking data  |  • Executive sponsor<br>• Finance team<br>• Steering committee<br>• Investment board<br>• Program manager  |  • Business case document<br>• Financial model (Excel)<br>• Executive summary<br>• Options matrix  |  • Only showing benefits, not costs<br>• No do-nothing option<br>• Ignoring intangible benefits<br>• Unrealistic timelines<br>• No risk to benefits  |  ☐ Options include do-nothing<br>☐ All costs included (TCO)<br>☐ Benefits quantified where possible<br>☐ Financial metrics calculated (NPV, ROI)<br>☐ Risks assessed  |  • For mandatory compliance projects (still track costs)<br>• For very small investments below approval threshold<br>• When business case already exists  |
|  **Definition of Ready**  |  Stories not ready for sprint; Development blocked; Requirements incomplete; Team pulls unprepared work; Velocity suffers  |  Checklist of criteria that user stories/backlog items must meet before being pulled into a sprint or development cycle  |  • INVEST criteria (Independent, Negotiable, Valuable, Estimable, Small, Testable)<br>• Acceptance criteria defined<br>• Dependencies resolved<br>• Designs available<br>• Estimate agreed<br>• Team understanding confirmed  |  • Agreed by whole team<br>• Consistently applied<br>• Reviewed regularly<br>• Visible and accessible<br>• Enforced at sprint planning<br>• Reduces mid-sprint surprises  |  • Team retrospectives<br>• Sprint planning feedback<br>• Historical blockers<br>• DoD as reference<br>• Team working agreements  |  • Product Owner<br>• Scrum Master<br>• Development team<br>• UX team<br>• QA team  |  • Checklist (wiki, board)<br>• Story template with mandatory fields<br>• Confluence page<br>• Team room poster  |  • Too many criteria (bureaucracy)<br>• Not enforced<br>• Criteria too vague<br>• Created without team<br>• Never updated  |  ☐ Agreed by whole team<br>☐ Criteria are checkable<br>☐ Applied consistently<br>☐ Under 10 items<br>☐ Reviewed last quarter  |  • When team very experienced and self-organizing<br>• For prototyping/spike work<br>• When specifications are always complete  |
|  **Test Concept**  |  Test strategy unclear; Coverage gaps; Test types undefined; Environment needs unknown; Quality risks unmitigated  |  Comprehensive test strategy document defining scope, approach, types, levels, environment, data, schedule, and exit criteria  |  • Test scope and objectives<br>• Test levels (unit/integration/system/acceptance)<br>• Test types (functional/NFR/regression)<br>• Test environment requirements<br>• Test data strategy<br>• Entry/Exit criteria<br>• Roles and responsibilities  |  • Comprehensive coverage<br>• Risk-based prioritization<br>• Environment defined<br>• Roles clear<br>• Aligned with schedule<br>• Approved by QA lead  |  • Requirements specs<br>• Architecture docs<br>• NFR catalog<br>• Risk register<br>• Project schedule<br>• Quality standards  |  • QA team<br>• Test managers<br>• Developers<br>• DevOps team<br>• Project manager  |  • Test plan document (IEEE 829)<br>• Test strategy<br>• Test approach matrix<br>• Test pyramid diagram  |  • Only functional testing<br>• No NFR testing<br>• Environment as afterthought<br>• No automation strategy<br>• Ignoring test data needs  |  ☐ All test levels defined<br>☐ NFR testing included<br>☐ Environment specified<br>☐ Automation strategy clear<br>☐ Approved by QA lead  |  • For trivial changes<br>• When following existing test strategy<br>• For proof-of-concept work  |
|  **Release Plan**  |  Release timing unclear; Scope uncertainty; Dependency coordination lacking; Rollback plan missing; Go-live chaos  |  Coordinated schedule for delivering product increments, defining scope per release, dependencies, milestones, go-live activities, and rollback procedures  |  • Release schedule and milestones<br>• Scope per release<br>• Dependencies and sequencing<br>• Go-live activities checklist<br>• Rollback procedures<br>• Success metrics per release  |  • Realistic timeline<br>• Dependencies managed<br>• Risk mitigation planned<br>• Stakeholders aligned<br>• Rollback defined<br>• Success metrics clear  |  • Product roadmap<br>• Story map<br>• Backlog<br>• Architecture constraints<br>• Team capacity<br>• Dependency maps  |  • Product Owner<br>• Development team<br>• Release manager<br>• Operations<br>• Stakeholders  |  • Release calendar<br>• Roadmap (timeline)<br>• Release checklist<br>• Communication plan  |  • Too much scope in MVP<br>• No incremental value<br>• Dependencies ignored<br>• No rollback plan<br>• Dates without capacity check  |  ☐ Each release delivers value<br>☐ Dependencies sequenced<br>☐ Timeline realistic for capacity<br>☐ Go-live checklist defined<br>☐ Rollback plan documented  |  • When releasing continuously with CI/CD<br>• For single-release projects<br>• When release process is fully standardized  |

---

## How to Use This Catalog

1. **Select Deliverable** - Choose the artifact that matches your current BA task
2. **Review Context** - Understand problems solved, inputs, and consumers
3. **Check Quality Criteria** - Know what "good" looks like
4. **Apply AI Acceleration** - Use the concrete prompts and automation ideas to accelerate your work
5. **Validate Quality** - Use the review checklist before finalizing
6. **Avoid Pitfalls** - Check typical mistakes and when NOT to use

## AI Acceleration Patterns

The AI techniques in this catalog follow these proven patterns:

- **"Generate X from Y"** - Draft creation from source materials
- **"Review for gaps against checklist"** - Quality validation
- **"Convert meeting transcript → artifact"** - Meeting notes extraction
- **"Create Mermaid/BPMN from description"** - Diagram generation
- **"Create acceptance criteria from story"** - Derivation and expansion
- **Consistency checks between artifacts** - Cross-artifact validation
- **Test case generation** - From requirements to tests
- **Traceability creation** - Link requirements to implementation

## Key Principles

- **Practitioner Language**: Uses BABOK, IREB, and Agile terminology
- **Actionable Components**: Each element can be directly executed
- **Executable AI Ideas**: Prompts are copy-paste ready with clear inputs/outputs
- **Quality Focus**: Each deliverable includes quality criteria and review checklists
- **Context Awareness**: Guidance on when to use and when NOT to use each artifact

---

*This catalog is a living document. As AI capabilities evolve and new patterns emerge, update the acceleration techniques to reflect current best practices.*
